\documentclass{article}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{arrows,intersections}
\usetikzlibrary{decorations.markings}

\usepackage{amsmath} % Required for \varPsi below

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{graphicx}
\usepackage{float}
\usepackage{rotating}

\begin{document}

\title{Titanic - ML Project}
\author{Bogdanowicz Michal Kamil, Geraci Luca}

\maketitle

\begin{abstract}
Report of the project for the course of ML 2019/2020
\end{abstract}


\section{Proposition}
The proposition is using a set of machine learning methods to predict the people that would survive the Titanic sinking of 15 April 1912.

The methods that have been used are:

\begin{enumerate}  
\item Logistic Regression TODO
\item Decision trees and Random Forest
\item Neural Networks TODO
\end{enumerate}

\section{Data}
\subsection{Incomplete Data}

The Data propsed to the professor was incomplete. It heavily reduced the avialble data to perform the best practices for evaluating a model. The test and training were already separated and ground truth was missing. So the complete dataset has been taken from the internet. In addition the complete list of surviors can be found at the wikipedia page \href{https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic}{here} (not a ML-friendly format).
This data sadly is used to cheat on the kaggle competition. Making it a quite infamous one.

\subsection{Data Format}

\begin{itemize}
\item Ticket class
\item Survival 
\item Name
\item Sex
\item Age in years
\item sibsp : number of siblings / spouses aboard the Titanic <-- This might be problematic as it doesn't seem to make much sense.
\item parch : number of parents / children aboard the Titanic<-- This might be problematic as it doesn't seem to make much sense.
\item Ticket number	
\item Fare
\item Cabin/s assigned
\item Port of Embarkation	
\item Rescue Boat
\item Body
\item Destination
\end{itemize}

With additional notes of :
Ticket class: A proxy for socio-economic status (SES)
1st = Upper
2nd = Middle
3rd = Lower

age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

sibsp: The dataset defines family relations in this way
Sibling = brother, sister, stepbrother, stepsister
Spouse = husband, wife (mistresses and fiancÃ©s were ignored)

parch: The dataset defines family relations in this way
Parent = mother, father
Child = daughter, son, stepdaughter, stepson
Some children travelled only with a nanny, therefore parch=0 for them.



\subsection{Data Preparation}
The data cannot be used as it is.
The names are going to be deleted, as that should not inlfuence the resut.

\section{Logistic Regression}
\subsection{Remark}

Since a lot of the data is a binary variable (0,1). The square of 0 and 1 comes back to the same value. That means that eleveting features which have only values 0 and 1 adds nothing to the model. Even more, it adds a fully correlated feature. Which has a negative impact on the model (even though it has not been the case empirically for this model. The reasons seems to be that the ensuring overestimation has no effect on the final result. The reasoning used still stands : Avoid correlated features for ML).
\\
\subsection{Procedure}

Procedure used for selecting the Logistic Regression model.

\begin{enumerate}  
\item Tthe maximum polynomial wase chosen by comparing the measures obtained with Cross Valdation (CV) methodology. The data can be seen in Table
\ref{tab:LR-Measures}. Maximim degree 2 has prevailed in most iterations. At each iteration the folds are randomized, and maximum degree 1 rarely beat 2.

\begin{table}[]
\centering
\caption{Polynomial degree measures comparison}
\label{tab:LR-Measures}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Measure/Degree  & 1     & \textbf{2}     & 3     & 4     & 5     \\ \hline
Accuracy  		& 0.804 & \textbf{0.810} & 0.717 & 0.662 & 0.437 \\ \hline
Precision		& 0.754 & \textbf{0.760} & 0.683 & 0.631 & 0.399 \\ \hline
Recall    		& 0.728 & \textbf{0.738} & 0.436 & 0.286 & 0.896 \\ \hline
F1        		& 0.738 & \textbf{0.746} & 0.515 & 0.390 & 0.534 \\ \hline
MAE       		& 0.196 & \textbf{0.190} & 0.283 & 0.338 & 0.563 \\ \hline
MSE       		& 0.196 & \textbf{0.190} & 0.283 & 0.338 & 0.563 \\ \hline
RAE       		& 0.414 & \textbf{0.403} & 0.599 & 0.715 & 1.193 \\ \hline
RSE       		& 0.828 & \textbf{0.806} & 1.197 & 1.430 & 2.385 \\ \hline
\end{tabular}
\end{table}

\item Bias and variance was checkd by visualizing the Error in CV and Error on Training set on a graph with X as the degree of polynomial. As it can be seen in the Graph \ref{Graph:LR-BV}, the cv and test MAE are very close. No Variance or Bias detected. 

\begin{figure}
\centering
\begin{tikzpicture}[y=10cm, x=2.5cm]

% horizontal axis
\draw[->] (1,0) -- (5,0) node[anchor=north] {};
% labels
\draw	(1,0) node[anchor=north] {1}
		(2,0) node[anchor=north] {2}
		(3,0) node[anchor=north] {3}
		(4,0) node[anchor=north] {4}
		(5,0) node[anchor=north] {5};

\draw	(1,0.1) node[anchor=east] {0.1}
		(1,0.2) node[anchor=east] {0.2}
		(1,0.3) node[anchor=east] {0.3}
		(1,0.4) node[anchor=east] {0.4}
		(1,0.5) node[anchor=east] {0.5}
		(1,0.6) node[anchor=east] {0.6};

% vertical axis
\draw[->] (1,0) -- (1,0.7) node[anchor=east] {MAE};

% Error Training
\draw plot [smooth] coordinates {(1,0.18352)  (2,0.18029)  (3,0.27298)   (4,0.33461)  (5, 0.58153)};

\draw (2,0.140) node {$J(\theta )_t$}; %label

% Error CV
\draw plot [smooth] coordinates {(1,0.19481)  (2,0.19101)  (3,0.27659)   (4,0.33535)  (5, 0.57908)};
\draw (1.5,0.237) node {$J(\theta )_{cv}$}; %label

\end{tikzpicture}
\label{Graph:LR-BV}
\caption{Plot of Errors with CV and Training}
\end{figure}

\item Quality of the model was controlled with the contruction of the learning curve in \ref{Graph:LR-LearningCurve} to see if there was variance and more training samples were required. They were not, as the algotihm used in octave iteself states around 200 iteration that the step become too small to be worth consideration.
\end{enumerate}

\begin{figure}
\centering
\begin{tikzpicture}[y=10cm, x=0.05cm]

% horizontal axis
\draw[->] (1,0) -- (200,0) node[anchor=north] {};
% labels
\draw	(0,0) node[anchor=north] {0}
		(50,0) node[anchor=north] {50}
		(100,0) node[anchor=north] {100}
		(150,0) node[anchor=north] {150}
		(200,0) node[anchor=north] {200};

\draw	(1,0.1) node[anchor=east] {0.1}
		(1,0.2) node[anchor=east] {0.2}
		(1,0.3) node[anchor=east] {0.3}
		(1,0.4) node[anchor=east] {0.4}
		(1,0.5) node[anchor=east] {0.5}
		(1,0.6) node[anchor=east] {0.6};

% vertical axis
\draw[->] (1,0) -- (1,0.7) node[anchor=east] {MAE};

% Error Training
\draw (100,0.15) node {$J(\theta )_t$}; %label

\draw plot [smooth] coordinates {(5,0.61803)  (10,0.36619)  (25,0.34233)   (50,0.28343)  (100, 0.18589)(125, 0.18267)(135, 0.18216)(150, 0.18139)(190, 0.18088)};

% Error CV

\draw (100,0.260) node {$J(\theta )_{cv}$}; %label

\draw plot [smooth] coordinates {(5,0.61803)  (10,0.37738)  (25,0.34228)   (50,0.27804)  (100, 0.22609)(125, 0.18561)(135, 0.18868)(150, 0.18792)(190, 0.18867)};

\end{tikzpicture}
\label{Graph:LR-LearningCurve}
\caption{Logistic Regression Learning Curve for Max degree pol. == 2}
\end{figure}

\subsection{Chosen model.}
Every feature + other 4 features containing elevated to the second power were chosen. With 400 max training samples ( default for octave, it would stop by himself having reached the limit around 200, when the adjustement step would be meaningless).

\section{Decision trees and random forest}
\subsection{Introduction to decision trees}
A decision tree builds upon iteratively asking questions to partition data.
Our aim is to increase the predictiveness of the model as much as possible at each partitioning so that the model keeps gaining information about the dataset.
There are two ways to measure the quality of a split:
\begin{enumerate}
	\item Gini-Index: The measure is about the impurity of a node. The aim is to reduce the impurity (reduce randomness) of data to achieve correct classification (labeling).
	\item Information Gain (Entropy):
	The information gain is the difference between entropy before and after the split.
	When splitting decision trees try to be more predictive, less impure and reduce entropy. Entropy is a measure of uncertainty or randomness. The more randomness a variable (feature) has, the higher the entropy is.  
\end{enumerate}

\subsection{Introduction to random forests}
Random forest is an ensemble of many decision trees. Random forests are built using a method called bagging in which each decision tree is used as parallel estimator.
Random forests reduce the risk of overfitting and accuracy is much higher than a single decision tree. Furthermore, decision trees in a random forest run in parallel so that the time does not become a bottleneck.


\subsection{Procedure}
We used the M5PrimeLab Octave toolbox as follows:
\begin{enumerate}  
	\item Normalize data introducing Dummy Variables and transform feature in numerical data
	\item Build and plot the decision tree to inspect generated number of rules
	\item Use 10-fold Cross Validation to calculate the key indicators MAE, MSE, RMSE, RRMSE, R2, nRules, nVars
	\item Build and plot the precision tree to inspect generated number of rules
	\item Use 10-fold Cross Validation on precision tree to calculate the key indicators MAE, MSE, RMSE, RRMSE, R2, nRules, nVars
	\item Calculate prediction, training set mean and input variable contributions
	\item Extract the decision rules from the tree
	\item Build tree forest (ensemble) 
	\item Calculate the out-of-bag Mean Squared Error (MSE) 
	\item Plot the variable importance
	\item Predict the forest
	\item Use 10-fold Cross Validation to evaluate the predictive performance of the forest
\end{enumerate}

\subsection{Results}

\newpage
\vfill
\begin{sidewaysfigure}[h]
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{decision_tree.png}
	\caption{decision tree}
\end{sidewaysfigure}	
\begin{sidewaysfigure}[h]
	\centering
	\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{precision_tree.png}
	\caption{precision tree}
\end{sidewaysfigure}
\vfill
\clearpage


	
\subsection{Lesson learned}
\subsubsection{Advantages of Decision Trees}
\begin{enumerate}
	\item No normalization or scaling of features
	\item Suitable for mixed feature data types
	\item Easy results interpretation
\end{enumerate}

\subsubsection{Disadvantages of Random Trees}
\begin{enumerate}
\item Prone to overfit and need to build forests to get good results
\end{enumerate}

\subsubsection{Advantages of Random Forests}
\begin{enumerate}
	\item Powerful, highly accurate model on many different problems
	\item No normalization or scaling of features
	\item Suitable for mixed feature data types
	\item Parallel computation for stable performance
\end{enumerate}

\subsubsection{Disadvantages of Random Forests}
\begin{enumerate}
	\item Not a good choice for high-dimensional data
\end{enumerate}


\section{Conclusion}
Write your conclusion here.

\end{document}
